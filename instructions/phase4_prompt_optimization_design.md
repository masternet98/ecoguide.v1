# Phase 4: í”„ë¡¬í”„íŠ¸ ìµœì í™” ì‹œìŠ¤í…œ ì„¤ê³„ì„œ

## ğŸ¯ ëª©ì  ë° ë²”ìœ„

### í•µì‹¬ ëª©ì 
Phase 1(í”¼ë“œë°±)ê³¼ Phase 3(RAG)ì˜ ì„±ê³¼ë¥¼ í†µí•©í•˜ì—¬ **ì»¨í…ìŠ¤íŠ¸ ì¸ì‹ ê³ í’ˆì§ˆ AI ì‘ë‹µ ìƒì„± ì‹œìŠ¤í…œ**ì„ êµ¬ì¶•í•˜ê³ , ì§€ì†ì ì¸ ê°œì„ ì„ í†µí•´ ì „ì²´ ì‹œìŠ¤í…œ ì™„ì„±ë„ ê·¹ëŒ€í™”

### ë¹„ì¦ˆë‹ˆìŠ¤ ê°€ì¹˜
- **ì‘ë‹µ í’ˆì§ˆ ê·¹ëŒ€í™”**: ì‚¬ìš©ìë³„, ìƒí™©ë³„ ë§ì¶¤í˜• ê³ í’ˆì§ˆ ì‘ë‹µ
- **ì§€ì†ì  ìê°€ ê°œì„ **: A/B í…ŒìŠ¤íŠ¸ì™€ í”¼ë“œë°± ê¸°ë°˜ ìë™ ìµœì í™”
- **ì‹œìŠ¤í…œ ì™„ì„±ë„**: ëª¨ë“  Phaseì˜ ì‹œë„ˆì§€ë¥¼ í†µí•œ ì™„ì „í•œ AI ì„œë¹„ìŠ¤
- **ê²½ìŸ ìš°ìœ„ í™•ë³´**: ë‹¨ìˆœ ì •ë³´ ì œê³µì„ ë„˜ì–´ì„  ì§€ëŠ¥í˜• ìƒë‹´ ì„œë¹„ìŠ¤

### êµ¬í˜„ ë²”ìœ„
- âœ… ì»¨í…ìŠ¤íŠ¸ ì¸ì‹ í”„ë¡¬í”„íŠ¸ ìƒì„±
- âœ… A/B í…ŒìŠ¤íŠ¸ ì‹œìŠ¤í…œ
- âœ… ì‘ë‹µ í’ˆì§ˆ ìë™ í‰ê°€
- âœ… í”„ë¡¬í”„íŠ¸ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
- âœ… í”¼ë“œë°± ê¸°ë°˜ ì§€ì†ì  ê°œì„ 
- âœ… ê°œì¸í™” í”„ë¡¬í”„íŠ¸ ì „ëµ

## ğŸ”— ê¸°ì¡´ ì‹œìŠ¤í…œ ì—°ê³„ì 

### ì£¼ìš” ì˜ì¡´ì„±
```python
# Phase 1 ì—°ê³„ (í•„ìˆ˜)
- FeedbackService: ì‚¬ìš©ì í”¼ë“œë°± íŒ¨í„´ ë¶„ì„
- AccuracyTrackingService: ì„±ëŠ¥ ì§€í‘œ í™œìš©

# Phase 3 ì—°ê³„ (í•„ìˆ˜)
- DistrictRAGService: ì§€ìì²´ ì •ë³´ ì»¨í…ìŠ¤íŠ¸
- RAGSearchService: ê²€ìƒ‰ í’ˆì§ˆ ë°ì´í„°

# ê¸°ì¡´ ì‹œìŠ¤í…œ
- OpenAIService: LLM ì‘ë‹µ ìƒì„±
- PromptService: ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ ê´€ë¦¬
```

### ë…ë¦½ ì‹¤í–‰ ì‹œ ì œí•œì‚¬í•­
```python
# Phase 1+3 ì—†ì´ëŠ” ê¸°ëŠ¥ ì œí•œì 
- ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ ìµœì í™”ë§Œ ê°€ëŠ¥
- ê°œì¸í™” ë° ì»¨í…ìŠ¤íŠ¸ ì¸ì‹ ê¸°ëŠ¥ ì œí•œ
- ë°ì´í„° ê¸°ë°˜ ê°œì„  íš¨ê³¼ ì œí•œì 
```

## ğŸ—ï¸ ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜

### ì»´í¬ë„ŒíŠ¸ êµ¬ì¡°
```
src/
â”œâ”€â”€ domains/
â”‚   â””â”€â”€ prompts/
â”‚       â””â”€â”€ services/
â”‚           â”œâ”€â”€ context_aware_prompt_service.py  # ì»¨í…ìŠ¤íŠ¸ ì¸ì‹ í”„ë¡¬í”„íŠ¸
â”‚           â”œâ”€â”€ prompt_optimization_service.py   # í”„ë¡¬í”„íŠ¸ ìµœì í™” ê´€ë¦¬
â”‚           â”œâ”€â”€ ab_test_service.py              # A/B í…ŒìŠ¤íŠ¸ ê´€ë¦¬
â”‚           â”œâ”€â”€ response_quality_service.py     # ì‘ë‹µ í’ˆì§ˆ í‰ê°€
â”‚           â””â”€â”€ personalization_service.py      # ê°œì¸í™” í”„ë¡¬í”„íŠ¸
â””â”€â”€ pages/
    â””â”€â”€ prompt_optimization_dashboard.py        # ìµœì í™” ëŒ€ì‹œë³´ë“œ (ê´€ë¦¬ììš©)
```

### í”„ë¡¬í”„íŠ¸ ìµœì í™” í”Œë¡œìš°
```
ì‚¬ìš©ì ìš”ì²­
    â†“
Context Analysis (ì»¨í…ìŠ¤íŠ¸ ë¶„ì„)
    â†“
Prompt Strategy Selection (ì „ëµ ì„ íƒ)
    â†“
Dynamic Prompt Generation (ë™ì  í”„ë¡¬í”„íŠ¸ ìƒì„±)
    â†“
A/B Test Assignment (A/B í…ŒìŠ¤íŠ¸ í• ë‹¹)
    â†“
LLM Response Generation (ì‘ë‹µ ìƒì„±)
    â†“
Quality Assessment (í’ˆì§ˆ í‰ê°€)
    â†“
Performance Tracking (ì„±ëŠ¥ ì¶”ì )
    â†“
Continuous Improvement (ì§€ì†ì  ê°œì„ )
```

## ğŸ“‹ ì„¸ë¶€ ê¸°ëŠ¥ ì„¤ê³„

### 1. ContextAwarePromptService (í•µì‹¬ í”„ë¡¬í”„íŠ¸ ì—”ì§„)

```python
# src/domains/prompts/services/context_aware_prompt_service.py
from src.app.core.base_service import BaseService
from typing import Dict, List, Optional, Any
from dataclasses import dataclass
from enum import Enum

class PromptStrategy(Enum):
    BEGINNER_FRIENDLY = "beginner_friendly"
    TECHNICAL_DETAILED = "technical_detailed"
    QUICK_ANSWER = "quick_answer"
    COMPREHENSIVE = "comprehensive"
    VISUAL_FOCUSED = "visual_focused"

@dataclass
class PromptContext:
    """í”„ë¡¬í”„íŠ¸ ìƒì„±ì„ ìœ„í•œ ì»¨í…ìŠ¤íŠ¸ ì •ë³´"""
    # ì‚¬ìš©ì ì •ë³´
    user_session_id: str
    user_history: Dict[str, Any]
    user_expertise_level: str  # 'beginner', 'intermediate', 'expert'

    # ë¶„ì„ ê²°ê³¼ ì •ë³´
    vision_result: Dict[str, Any]
    confidence_level: float

    # RAG ê²€ìƒ‰ ê²°ê³¼
    rag_context: Dict[str, Any]
    district_info: Dict[str, Any]

    # ìƒí™© ì •ë³´
    time_of_day: str
    device_type: str  # 'mobile', 'desktop'
    session_duration: float

    # í”¼ë“œë°± ì´ë ¥
    previous_feedback: List[Dict[str, Any]]
    accuracy_history: Dict[str, float]

class ContextAwarePromptService(BaseService):
    """ì»¨í…ìŠ¤íŠ¸ ì¸ì‹ í”„ë¡¬í”„íŠ¸ ìƒì„± ì„œë¹„ìŠ¤"""

    def __init__(self, config):
        super().__init__(config)
        self.prompt_templates = self._load_prompt_templates()
        self.optimization_service = None  # lazy loading

    def generate_contextual_prompt(self, context: PromptContext) -> Dict[str, Any]:
        """ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ìµœì  í”„ë¡¬í”„íŠ¸ ìƒì„± (ë©”ì¸ API)"""

        try:
            # 1. ì‚¬ìš©ì í”„ë¡œí•„ ë¶„ì„
            user_profile = self._analyze_user_profile(context)

            # 2. ìµœì  ì „ëµ ì„ íƒ
            optimal_strategy = self._select_optimal_strategy(context, user_profile)

            # 3. ë™ì  í”„ë¡¬í”„íŠ¸ ìƒì„±
            prompt_components = self._build_prompt_components(context, optimal_strategy)

            # 4. A/B í…ŒìŠ¤íŠ¸ ë³€í˜• ì ìš© (ì„ íƒì )
            final_prompt = self._apply_ab_test_variation(prompt_components, context)

            # 5. í”„ë¡¬í”„íŠ¸ ë©”íƒ€ë°ì´í„° ìƒì„±
            metadata = self._generate_prompt_metadata(context, optimal_strategy, user_profile)

            return {
                'success': True,
                'prompt': final_prompt,
                'strategy': optimal_strategy.value,
                'user_profile': user_profile,
                'metadata': metadata,
                'expected_quality': self._predict_response_quality(context, optimal_strategy)
            }

        except Exception as e:
            self.logger.error(f"Context-aware prompt generation failed: {e}")
            return self._generate_fallback_prompt(context)

    def _analyze_user_profile(self, context: PromptContext) -> Dict[str, Any]:
        """ì‚¬ìš©ì í”„ë¡œí•„ ë¶„ì„"""

        profile = {
            'expertise_level': context.user_expertise_level,
            'communication_preference': 'standard',
            'attention_span': 'medium',
            'detail_preference': 'balanced',
            'confidence_in_system': 0.5
        }

        # ì‚¬ìš©ì íˆìŠ¤í† ë¦¬ ê¸°ë°˜ ë¶„ì„
        if context.user_history:
            # ì´ì „ ì„¸ì…˜ì—ì„œì˜ ì„ í˜¸ë„ íŒ¨í„´ ë¶„ì„
            profile.update(self._extract_preferences_from_history(context.user_history))

        # í”¼ë“œë°± íŒ¨í„´ ë¶„ì„
        if context.previous_feedback:
            feedback_analysis = self._analyze_feedback_patterns(context.previous_feedback)
            profile.update(feedback_analysis)

        # ì •í™•ë„ ì´ë ¥ ê¸°ë°˜ ì‹ ë¢°ë„ ì¡°ì •
        if context.accuracy_history:
            avg_accuracy = sum(context.accuracy_history.values()) / len(context.accuracy_history)
            profile['confidence_in_system'] = avg_accuracy

        # ë””ë°”ì´ìŠ¤ ê¸°ë°˜ ì¡°ì •
        if context.device_type == 'mobile':
            profile['attention_span'] = 'short'
            profile['detail_preference'] = 'concise'

        return profile

    def _select_optimal_strategy(self, context: PromptContext, user_profile: Dict) -> PromptStrategy:
        """ìµœì  í”„ë¡¬í”„íŠ¸ ì „ëµ ì„ íƒ"""

        # ê¸°ë³¸ ì ìˆ˜ ê³„ì‚°
        strategy_scores = {
            PromptStrategy.BEGINNER_FRIENDLY: 0.3,
            PromptStrategy.TECHNICAL_DETAILED: 0.2,
            PromptStrategy.QUICK_ANSWER: 0.2,
            PromptStrategy.COMPREHENSIVE: 0.2,
            PromptStrategy.VISUAL_FOCUSED: 0.1
        }

        # ì‚¬ìš©ì ì „ë¬¸ì„± ìˆ˜ì¤€ ë°˜ì˜
        if user_profile['expertise_level'] == 'beginner':
            strategy_scores[PromptStrategy.BEGINNER_FRIENDLY] += 0.4
            strategy_scores[PromptStrategy.VISUAL_FOCUSED] += 0.2
        elif user_profile['expertise_level'] == 'expert':
            strategy_scores[PromptStrategy.TECHNICAL_DETAILED] += 0.4
            strategy_scores[PromptStrategy.COMPREHENSIVE] += 0.2

        # ì‹ ë¢°ë„ ê¸°ë°˜ ì¡°ì •
        confidence = context.confidence_level
        if confidence > 0.8:
            strategy_scores[PromptStrategy.COMPREHENSIVE] += 0.2
        elif confidence < 0.5:
            strategy_scores[PromptStrategy.BEGINNER_FRIENDLY] += 0.3

        # RAG ì»¨í…ìŠ¤íŠ¸ í’ˆì§ˆ ë°˜ì˜
        rag_confidence = context.rag_context.get('confidence_score', 0.5)
        if rag_confidence > 0.8:
            strategy_scores[PromptStrategy.TECHNICAL_DETAILED] += 0.2
        else:
            strategy_scores[PromptStrategy.QUICK_ANSWER] += 0.2

        # ë””ë°”ì´ìŠ¤ íƒ€ì… ë°˜ì˜
        if context.device_type == 'mobile':
            strategy_scores[PromptStrategy.QUICK_ANSWER] += 0.3
            strategy_scores[PromptStrategy.VISUAL_FOCUSED] += 0.2

        # ì‹œê°„ëŒ€ ë°˜ì˜ (ë°¤ëŠ¦ì€ ì‹œê°„ = ê°„ë‹¨í•œ ë‹µë³€ ì„ í˜¸)
        if context.time_of_day in ['late_night', 'early_morning']:
            strategy_scores[PromptStrategy.QUICK_ANSWER] += 0.2

        # ìµœê³  ì ìˆ˜ ì „ëµ ì„ íƒ
        return max(strategy_scores, key=strategy_scores.get)

    def _build_prompt_components(self, context: PromptContext, strategy: PromptStrategy) -> Dict[str, str]:
        """ì „ëµë³„ í”„ë¡¬í”„íŠ¸ ì»´í¬ë„ŒíŠ¸ êµ¬ì„±"""

        components = {
            'system_role': self._get_system_role(strategy),
            'context_section': self._build_context_section(context, strategy),
            'instruction_section': self._build_instruction_section(strategy),
            'output_format': self._get_output_format(strategy),
            'constraints': self._get_constraints(strategy, context)
        }

        return components

    def _get_system_role(self, strategy: PromptStrategy) -> str:
        """ì „ëµë³„ ì‹œìŠ¤í…œ ì—­í•  ì •ì˜"""

        role_templates = {
            PromptStrategy.BEGINNER_FRIENDLY: """ë‹¹ì‹ ì€ ì¹œê·¼í•˜ê³  ì´í•´í•˜ê¸° ì‰¬ìš´ ëŒ€í˜•íê¸°ë¬¼ ë°°ì¶œ ë„ìš°ë¯¸ì…ë‹ˆë‹¤.
ì´ˆë³´ìë„ ì‰½ê²Œ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ë‹¨ê³„ë³„ë¡œ ì°¨ê·¼ì°¨ê·¼ ì„¤ëª…í•´ì£¼ì„¸ìš”.""",

            PromptStrategy.TECHNICAL_DETAILED: """ë‹¹ì‹ ì€ ëŒ€í˜•íê¸°ë¬¼ ë°°ì¶œ ê·œì • ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì •í™•í•œ ë²•ê·œì™€ ì„¸ë¶€ ì ˆì°¨ë¥¼ í¬í•¨í•˜ì—¬ ì „ë¬¸ì ì´ê³  ìƒì„¸í•œ ì •ë³´ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”.""",

            PromptStrategy.QUICK_ANSWER: """ë‹¹ì‹ ì€ íš¨ìœ¨ì ì¸ ëŒ€í˜•íê¸°ë¬¼ ë°°ì¶œ ì•ˆë‚´ì›ì…ë‹ˆë‹¤.
í•µì‹¬ ì •ë³´ë§Œ ë¹ ë¥´ê³  ëª…í™•í•˜ê²Œ ì „ë‹¬í•´ì£¼ì„¸ìš”.""",

            PromptStrategy.COMPREHENSIVE: """ë‹¹ì‹ ì€ ì¢…í•©ì ì¸ ëŒ€í˜•íê¸°ë¬¼ ê´€ë¦¬ ì»¨ì„¤í„´íŠ¸ì…ë‹ˆë‹¤.
ì „ì²´ì ì¸ ë§¥ë½ê³¼ ë‹¤ì–‘í•œ ì˜µì…˜ì„ í¬í•¨í•˜ì—¬ ì™„ì „í•œ ì•ˆë‚´ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”.""",

            PromptStrategy.VISUAL_FOCUSED: """ë‹¹ì‹ ì€ ì‹œê°ì  ì„¤ëª…ì— íŠ¹í™”ëœ ëŒ€í˜•íê¸°ë¬¼ ë°°ì¶œ ê°€ì´ë“œì…ë‹ˆë‹¤.
ì´ë¯¸ì§€ ë¶„ì„ ê²°ê³¼ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ ì§ê´€ì ì´ê³  ì´í•´í•˜ê¸° ì‰½ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”."""
        }

        return role_templates.get(strategy, role_templates[PromptStrategy.BEGINNER_FRIENDLY])

    def _build_context_section(self, context: PromptContext, strategy: PromptStrategy) -> str:
        """ì»¨í…ìŠ¤íŠ¸ ì„¹ì…˜ êµ¬ì„±"""

        context_parts = []

        # ê¸°ë³¸ ìƒí™© ì •ë³´
        context_parts.append(f"=== í˜„ì¬ ìƒí™© ===")
        context_parts.append(f"ì‚¬ìš©ì ìœ„ì¹˜: {context.district_info.get('district_name', 'ì •ë³´ ì—†ìŒ')}")

        # ì´ë¯¸ì§€ ë¶„ì„ ê²°ê³¼
        vision_info = context.vision_result
        if vision_info:
            confidence_emoji = "ğŸ”´" if vision_info.get('confidence', 0) < 0.5 else "ğŸŸ¡" if vision_info.get('confidence', 0) < 0.8 else "ğŸŸ¢"
            context_parts.append(f"ê°ì§€ëœ ë¬¼ê±´: {vision_info.get('category', 'ë¯¸í™•ì¸')} {confidence_emoji}")

            if strategy in [PromptStrategy.TECHNICAL_DETAILED, PromptStrategy.COMPREHENSIVE]:
                context_parts.append(f"ë¶„ì„ ì‹ ë¢°ë„: {vision_info.get('confidence', 0):.1%}")
                size_info = vision_info.get('size_estimation', {})
                if size_info:
                    context_parts.append(f"ì¶”ì • í¬ê¸°: {size_info.get('width_cm', 0):.1f}cm Ã— {size_info.get('height_cm', 0):.1f}cm")

        # RAG ê²€ìƒ‰ ê²°ê³¼ (ê³ í’ˆì§ˆì¸ ê²½ìš°ë§Œ)
        rag_info = context.rag_context
        if rag_info and rag_info.get('confidence_score', 0) > 0.6:
            context_parts.append(f"\n=== ì§€ìì²´ ê³µì‹ ì •ë³´ ===")
            if strategy != PromptStrategy.QUICK_ANSWER:
                context_parts.append(rag_info.get('context_window', ''))
            else:
                # ê°„ë‹¨í•œ ë‹µë³€ ì „ëµì—ì„œëŠ” í•µì‹¬ ì •ë³´ë§Œ
                key_info = self._extract_key_from_rag(rag_info)
                context_parts.append(key_info)

        # ì‚¬ìš©ì í”¼ë“œë°± ì´ë ¥ (ê´€ë ¨ì„± ìˆëŠ” ê²½ìš°ë§Œ)
        if context.previous_feedback and strategy in [PromptStrategy.COMPREHENSIVE, PromptStrategy.TECHNICAL_DETAILED]:
            recent_feedback = context.previous_feedback[-3:]  # ìµœê·¼ 3ê°œë§Œ
            if recent_feedback:
                context_parts.append(f"\n=== ì´ì „ í”¼ë“œë°± íŒ¨í„´ ===")
                for feedback in recent_feedback:
                    if feedback.get('classification_correct') == False:
                        context_parts.append(f"- {feedback.get('corrected_classification', '')} ë¶„ë¥˜ ìˆ˜ì • ì´ë ¥")

        return "\n".join(context_parts)

    def _build_instruction_section(self, strategy: PromptStrategy) -> str:
        """ì§€ì‹œì‚¬í•­ ì„¹ì…˜ êµ¬ì„±"""

        instruction_templates = {
            PromptStrategy.BEGINNER_FRIENDLY: """
ë‹¤ìŒ ìˆœì„œë¡œ ì¹œê·¼í•˜ê²Œ ì•ˆë‚´í•´ì£¼ì„¸ìš”:
1. ğŸ˜Š ê°ì§€ëœ ë¬¼ê±´ì— ëŒ€í•œ í™•ì¸ ë° ê°„ë‹¨í•œ ì„¤ëª…
2. ğŸ’° í•´ë‹¹ ì§€ì—­ì˜ ìˆ˜ìˆ˜ë£Œ ì •ë³´ (êµ¬ì²´ì  ê¸ˆì•¡)
3. ğŸ“… ë°°ì¶œ ê°€ëŠ¥í•œ ë‚ ì§œì™€ ì‹œê°„
4. ğŸ“‹ ì‹ ì²­ ë˜ëŠ” ë°°ì¶œ ë°©ë²• (ë‹¨ê³„ë³„)
5. âš ï¸ ì£¼ì˜ì‚¬í•­ (ì•ˆì „ ê´€ë ¨)
6. ğŸ“ ê¶ê¸ˆí•œ ì ì´ ìˆì„ ë•Œ ë¬¸ì˜í•  ê³³

ê° ë‹¨ê³„ë§ˆë‹¤ ì´ëª¨ì§€ë¥¼ ì‚¬ìš©í•˜ê³ , ì´í•´í•˜ê¸° ì‰¬ìš´ ë§ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”.""",

            PromptStrategy.TECHNICAL_DETAILED: """
ë‹¤ìŒ í•­ëª©ë“¤ì„ ì •í™•í•˜ê³  ìƒì„¸í•˜ê²Œ ì•ˆë‚´í•´ì£¼ì„¸ìš”:
1. ë¬¼ê±´ ë¶„ë¥˜ ë° ê´€ë ¨ ë²•ê·œ/ì¡°ë¡€
2. ìˆ˜ìˆ˜ë£Œ ê³„ì‚° ë°©ì‹ ë° ê·¼ê±° ì¡°ë¡€
3. ë°°ì¶œ ì¼ì • ë° ì¥ì†Œ (ìƒì„¸ ì£¼ì†Œ í¬í•¨)
4. ì‹ ì²­ ì ˆì°¨ ë° í•„ìš” ì„œë¥˜
5. í¬ê¸°/ë¬´ê²Œ ì œí•œ ë° ì˜ˆì™¸ ì‚¬í•­
6. ìœ„ë°˜ ì‹œ ê³¼íƒœë£Œ ë° ë²•ì  ì¡°ì¹˜
7. ê´€ë ¨ ì¡°ë¡€ ë° ê·œì • ì°¸ì¡°

ì „ë¬¸ ìš©ì–´ ì‚¬ìš©ì„ í—ˆìš©í•˜ë©°, ë²•ì  ê·¼ê±°ë¥¼ ëª…ì‹œí•´ì£¼ì„¸ìš”.""",

            PromptStrategy.QUICK_ANSWER: """
í•µì‹¬ ì •ë³´ë§Œ ê°„ê²°í•˜ê²Œ ì œê³µí•´ì£¼ì„¸ìš”:
â€¢ ìˆ˜ìˆ˜ë£Œ: [êµ¬ì²´ì  ê¸ˆì•¡]
â€¢ ë°°ì¶œì¼: [ìš”ì¼/ë‚ ì§œ]
â€¢ ì‹ ì²­: [ë°©ë²•]
â€¢ ë¬¸ì˜: [ì—°ë½ì²˜]

ì¶”ê°€ ì„¤ëª…ì€ ìµœì†Œí™”í•˜ê³  ì‹¤í–‰ ê°€ëŠ¥í•œ ì •ë³´ë§Œ ì œê³µí•´ì£¼ì„¸ìš”.""",

            PromptStrategy.COMPREHENSIVE: """
ì¢…í•©ì ì¸ ì•ˆë‚´ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”:
1. ìƒí™© ë¶„ì„ ë° ë¬¼ê±´ í™•ì¸
2. í•´ë‹¹ ì§€ì—­ ê·œì • ì „ì²´ ê°œìš”
3. ìˆ˜ìˆ˜ë£Œ ë° ë°°ì¶œ ì¼ì • ìƒì„¸ ì •ë³´
4. ë‹¤ì–‘í•œ ë°°ì¶œ ë°©ë²• ë° ì¥ë‹¨ì  ë¹„êµ
5. ëŒ€ì•ˆì  ì²˜ë¦¬ ë°©ë²• (ì¬í™œìš©, ê¸°ë¶€ ë“±)
6. ì˜ˆìƒë˜ëŠ” ë¬¸ì œì  ë° í•´ê²° ë°©ë²•
7. ê´€ë ¨ ì •ì±… ë™í–¥ ë° ë³€ê²½ ì‚¬í•­
8. ì¶”ê°€ ë„ì›€ì´ ë  ìˆ˜ ìˆëŠ” ìì›

ë§¥ë½ê³¼ ë°°ê²½ ì •ë³´ë¥¼ í¬í•¨í•˜ì—¬ ì™„ì „í•œ ì´í•´ë¥¼ ë„ì™€ì£¼ì„¸ìš”.""",

            PromptStrategy.VISUAL_FOCUSED: """
ì´ë¯¸ì§€ ë¶„ì„ ê²°ê³¼ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”:
1. ğŸ” "ì‚¬ì§„ì—ì„œ ë³´ì´ëŠ” ë¬¼ê±´ì€ [ë¬¼ê±´ëª…]ì…ë‹ˆë‹¤"
2. ğŸ“ í¬ê¸° ë¶„ì„ ê²°ê³¼ ë° ìˆ˜ìˆ˜ë£Œ ì˜í–¥
3. ğŸ¯ ì´ ë¬¼ê±´ì˜ íŠ¹ë³„í•œ ì²˜ë¦¬ ë°©ë²• (ìˆë‹¤ë©´)
4. ğŸ“¸ ë°°ì¶œ ì‹œ ì‚¬ì§„ ì´¬ì˜ íŒ (ì‹ ì²­ìš©)
5. âœ… ì˜¬ë°”ë¥¸ ë°°ì¶œ ìƒíƒœ vs âŒ ì˜ëª»ëœ ë°°ì¶œ ìƒíƒœ

ì‹œê°ì  ìš”ì†Œì™€ ì´ë¯¸ì§€ ë¶„ì„ ê²°ê³¼ë¥¼ ìµœëŒ€í•œ í™œìš©í•´ì£¼ì„¸ìš”."""
        }

        return instruction_templates.get(strategy, instruction_templates[PromptStrategy.BEGINNER_FRIENDLY])

    def _get_output_format(self, strategy: PromptStrategy) -> str:
        """ì¶œë ¥ í˜•ì‹ ì§€ì •"""

        format_templates = {
            PromptStrategy.BEGINNER_FRIENDLY: """
ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”:
- ì¹œê·¼í•œ ë§íˆ¬ ì‚¬ìš© (ì¡´ëŒ“ë§, í•˜ì§€ë§Œ ë”±ë”±í•˜ì§€ ì•Šê²Œ)
- ì´ëª¨ì§€ ì ê·¹ í™œìš©
- ë‹¨ë½ë³„ë¡œ ëª…í™•íˆ êµ¬ë¶„
- ì¤‘ìš”í•œ ì •ë³´ëŠ” **êµµê²Œ** í‘œì‹œ""",

            PromptStrategy.TECHNICAL_DETAILED: """
ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”:
- ê³µì‹ì ì´ê³  ì •í™•í•œ í‘œí˜„ ì‚¬ìš©
- ë²•ê·œ/ì¡°ë¡€ ë²ˆí˜¸ ëª…ì‹œ
- í‘œë‚˜ ëª©ë¡ í˜•íƒœë¡œ ì •ë¦¬
- ì°¸ì¡° ë¬¸ì„œë‚˜ ë§í¬ í¬í•¨""",

            PromptStrategy.QUICK_ANSWER: """
ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”:
- ë¶ˆí•„ìš”í•œ ì„¤ëª… ì œê±°
- í•µì‹¬ ì •ë³´ë§Œ ë‚˜ì—´
- ì§§ì€ ë¬¸ì¥ ì‚¬ìš©
- ì‹¤í–‰ ê°€ëŠ¥í•œ ì •ë³´ ìœ„ì£¼""",

            PromptStrategy.COMPREHENSIVE: """
ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”:
- ì„¹ì…˜ë³„ë¡œ ì²´ê³„ì  êµ¬ì„±
- ìƒì„¸í•œ ì„¤ëª…ê³¼ ë§¥ë½ ì œê³µ
- ë‹¤ì–‘í•œ ì‹œë‚˜ë¦¬ì˜¤ ê³ ë ¤
- ì¶”ê°€ ì°¸ê³  ìë£Œ ì œê³µ""",

            PromptStrategy.VISUAL_FOCUSED: """
ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”:
- ì´ë¯¸ì§€ ë¶„ì„ ê²°ê³¼ ìš°ì„  ì–¸ê¸‰
- ì‹œê°ì  ë¹„êµë‚˜ ì˜ˆì‹œ í™œìš©
- "ì‚¬ì§„ì—ì„œ ë³´ì‹œëŠ” ë°”ì™€ ê°™ì´..." ê°™ì€ í‘œí˜„ ì‚¬ìš©
- ë‹¨ê³„ë³„ ì‹œê°ì  ê°€ì´ë“œ ì œê³µ"""
        }

        return format_templates.get(strategy, "ìì—°ìŠ¤ëŸ½ê³  ë„ì›€ì´ ë˜ëŠ” í˜•íƒœë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”.")

    def _apply_ab_test_variation(self, components: Dict[str, str], context: PromptContext) -> str:
        """A/B í…ŒìŠ¤íŠ¸ ë³€í˜• ì ìš©"""

        # A/B í…ŒìŠ¤íŠ¸ ì„œë¹„ìŠ¤ì—ì„œ í˜„ì¬ í™œì„± í…ŒìŠ¤íŠ¸ í™•ì¸
        ab_test_service = self._get_ab_test_service()
        active_test = ab_test_service.get_active_test_for_user(context.user_session_id)

        if active_test:
            # í…ŒìŠ¤íŠ¸ ë³€í˜• ì ìš©
            variation = active_test['variation']
            components = self._apply_test_variation(components, variation)

        # ìµœì¢… í”„ë¡¬í”„íŠ¸ ì¡°í•©
        final_prompt = f"""
{components['system_role']}

{components['context_section']}

{components['instruction_section']}

{components['output_format']}

{components['constraints']}
"""

        return final_prompt.strip()

    def create_personalized_prompt(self, context: PromptContext, user_preferences: Dict) -> str:
        """ê°œì¸í™” í”„ë¡¬í”„íŠ¸ ìƒì„±"""

        # ì‚¬ìš©ì ì„ í˜¸ë„ ë°˜ì˜
        if user_preferences.get('communication_style') == 'formal':
            # ê²©ì‹ìˆëŠ” ë§íˆ¬
            pass
        elif user_preferences.get('communication_style') == 'casual':
            # ìºì£¼ì–¼í•œ ë§íˆ¬
            pass

        # ì •ë³´ ìƒì„¸ë„ ì¡°ì ˆ
        detail_level = user_preferences.get('detail_preference', 'medium')
        if detail_level == 'minimal':
            # ìµœì†Œí•œì˜ ì •ë³´
            pass
        elif detail_level == 'maximum':
            # ìµœëŒ€í•œ ìƒì„¸í•œ ì •ë³´
            pass

        return self.generate_contextual_prompt(context)['prompt']
```

### 2. ABTestService (A/B í…ŒìŠ¤íŠ¸ ê´€ë¦¬)

```python
# src/domains/prompts/services/ab_test_service.py
class ABTestService(BaseService):
    """A/B í…ŒìŠ¤íŠ¸ ê´€ë¦¬ ì„œë¹„ìŠ¤"""

    def __init__(self, config):
        super().__init__(config)
        self.active_tests = {}
        self.test_results = {}

    def create_ab_test(self,
                      test_name: str,
                      test_description: str,
                      variations: List[Dict],
                      target_metric: str,
                      duration_days: int = 7) -> Dict[str, Any]:
        """A/B í…ŒìŠ¤íŠ¸ ìƒì„±"""

        test_id = f"test_{test_name}_{int(time.time())}"

        test_config = {
            'test_id': test_id,
            'name': test_name,
            'description': test_description,
            'variations': variations,  # [{'name': 'control', 'prompt_changes': {}}, {'name': 'variant_a', 'prompt_changes': {}}]
            'target_metric': target_metric,  # 'user_satisfaction', 'response_accuracy', 'engagement'
            'start_date': datetime.now(),
            'end_date': datetime.now() + timedelta(days=duration_days),
            'traffic_split': self._calculate_traffic_split(len(variations)),
            'status': 'active',
            'participants': {},
            'results': {variation['name']: {'count': 0, 'metrics': {}} for variation in variations}
        }

        self.active_tests[test_id] = test_config

        # ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
        self._save_test_config(test_config)

        return {
            'success': True,
            'test_id': test_id,
            'config': test_config
        }

    def assign_user_to_test(self, user_session_id: str, test_id: str = None) -> Dict[str, Any]:
        """ì‚¬ìš©ìë¥¼ A/B í…ŒìŠ¤íŠ¸ì— í• ë‹¹"""

        if test_id:
            test = self.active_tests.get(test_id)
        else:
            # ê°€ì¥ ìš°ì„ ìˆœìœ„ ë†’ì€ í™œì„± í…ŒìŠ¤íŠ¸ ì„ íƒ
            test = self._get_priority_active_test()

        if not test:
            return {'assigned': False, 'variation': None}

        # ì‚¬ìš©ì í• ë‹¹ ë¡œì§ (í•´ì‹œ ê¸°ë°˜ ì¼ê´€ì„± ë³´ì¥)
        variation = self._assign_user_variation(user_session_id, test)

        # í• ë‹¹ ê¸°ë¡
        test['participants'][user_session_id] = {
            'variation': variation,
            'assigned_at': datetime.now(),
            'interactions': []
        }

        return {
            'assigned': True,
            'test_id': test['test_id'],
            'variation': variation,
            'test_name': test['name']
        }

    def record_test_interaction(self,
                              user_session_id: str,
                              test_id: str,
                              interaction_type: str,
                              metrics: Dict[str, Any]) -> None:
        """í…ŒìŠ¤íŠ¸ ìƒí˜¸ì‘ìš© ê¸°ë¡"""

        test = self.active_tests.get(test_id)
        if not test or user_session_id not in test['participants']:
            return

        participant = test['participants'][user_session_id]
        variation = participant['variation']

        # ìƒí˜¸ì‘ìš© ê¸°ë¡
        interaction = {
            'type': interaction_type,
            'timestamp': datetime.now(),
            'metrics': metrics
        }

        participant['interactions'].append(interaction)

        # í…ŒìŠ¤íŠ¸ ê²°ê³¼ ì—…ë°ì´íŠ¸
        self._update_test_results(test, variation, interaction)

    def analyze_test_results(self, test_id: str) -> Dict[str, Any]:
        """A/B í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë¶„ì„"""

        test = self.active_tests.get(test_id)
        if not test:
            return {'success': False, 'error': 'Test not found'}

        analysis = {
            'test_info': {
                'name': test['name'],
                'duration': (datetime.now() - test['start_date']).days,
                'total_participants': len(test['participants']),
                'target_metric': test['target_metric']
            },
            'variation_performance': {},
            'statistical_significance': {},
            'recommendations': []
        }

        # ê° ë³€í˜•ë³„ ì„±ëŠ¥ ë¶„ì„
        for variation_name, results in test['results'].items():
            if results['count'] > 0:
                avg_metrics = {
                    metric: sum(values) / len(values)
                    for metric, values in results['metrics'].items()
                    if values
                }

                analysis['variation_performance'][variation_name] = {
                    'participant_count': results['count'],
                    'average_metrics': avg_metrics
                }

        # í†µê³„ì  ìœ ì˜ì„± ê²€ì •
        analysis['statistical_significance'] = self._calculate_statistical_significance(test)

        # ì¶”ì²œì‚¬í•­ ìƒì„±
        analysis['recommendations'] = self._generate_test_recommendations(analysis)

        return analysis

    def _calculate_statistical_significance(self, test: Dict) -> Dict[str, Any]:
        """í†µê³„ì  ìœ ì˜ì„± ê³„ì‚°"""

        # ê°„ë‹¨í•œ t-ê²€ì • ë˜ëŠ” ì¹´ì´ì œê³± ê²€ì •
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” scipy.stats ë“± ì‚¬ìš©

        significance = {
            'is_significant': False,
            'p_value': 0.0,
            'confidence_level': 0.95,
            'sample_size_sufficient': False
        }

        # ìµœì†Œ í‘œë³¸ í¬ê¸° í™•ì¸
        total_participants = sum(len(variation['participants']) for variation in test['results'].values())
        significance['sample_size_sufficient'] = total_participants >= 100

        # p-value ê³„ì‚° (ê°„ë‹¨í•œ êµ¬í˜„)
        if significance['sample_size_sufficient']:
            # ì‹¤ì œ í†µê³„ ê²€ì • ë¡œì§ êµ¬í˜„
            significance['p_value'] = 0.05  # ì„ì‹œê°’
            significance['is_significant'] = significance['p_value'] < 0.05

        return significance
```

### 3. ResponseQualityService (ì‘ë‹µ í’ˆì§ˆ í‰ê°€)

```python
# src/domains/prompts/services/response_quality_service.py
class ResponseQualityService(BaseService):
    """ì‘ë‹µ í’ˆì§ˆ í‰ê°€ ì„œë¹„ìŠ¤"""

    def __init__(self, config):
        super().__init__(config)
        self.quality_metrics = {
            'factual_accuracy': 0.3,
            'completeness': 0.25,
            'clarity': 0.2,
            'relevance': 0.15,
            'user_satisfaction': 0.1
        }

    def evaluate_response_quality(self,
                                prompt: str,
                                response: str,
                                context: Dict[str, Any],
                                user_feedback: Dict = None) -> Dict[str, Any]:
        """ì‘ë‹µ í’ˆì§ˆ ì¢…í•© í‰ê°€"""

        evaluation = {
            'overall_score': 0.0,
            'detailed_scores': {},
            'quality_grade': 'unknown',
            'improvement_suggestions': [],
            'confidence': 0.0
        }

        try:
            # ê° ì§€í‘œë³„ í‰ê°€
            detailed_scores = {}

            detailed_scores['factual_accuracy'] = self._evaluate_factual_accuracy(
                response, context
            )

            detailed_scores['completeness'] = self._evaluate_completeness(
                prompt, response, context
            )

            detailed_scores['clarity'] = self._evaluate_clarity(response)

            detailed_scores['relevance'] = self._evaluate_relevance(
                prompt, response, context
            )

            if user_feedback:
                detailed_scores['user_satisfaction'] = self._evaluate_user_satisfaction(
                    user_feedback
                )
            else:
                detailed_scores['user_satisfaction'] = 0.5  # ì¤‘ê°„ê°’

            # ê°€ì¤‘ í‰ê·  ê³„ì‚°
            overall_score = sum(
                score * self.quality_metrics[metric]
                for metric, score in detailed_scores.items()
            )

            evaluation.update({
                'overall_score': round(overall_score, 3),
                'detailed_scores': detailed_scores,
                'quality_grade': self._determine_quality_grade(overall_score),
                'improvement_suggestions': self._generate_improvement_suggestions(detailed_scores),
                'confidence': self._calculate_evaluation_confidence(detailed_scores, user_feedback)
            })

        except Exception as e:
            self.logger.error(f"Response quality evaluation failed: {e}")
            evaluation['error'] = str(e)

        return evaluation

    def _evaluate_factual_accuracy(self, response: str, context: Dict) -> float:
        """ì‚¬ì‹¤ì  ì •í™•ì„± í‰ê°€"""

        score = 0.5  # ê¸°ë³¸ì ìˆ˜

        # RAG ì»¨í…ìŠ¤íŠ¸ì™€ì˜ ì¼ì¹˜ì„± í™•ì¸
        rag_context = context.get('rag_context', {})
        if rag_context:
            context_confidence = rag_context.get('confidence_score', 0.5)

            # ì‘ë‹µì´ RAG ì»¨í…ìŠ¤íŠ¸ì™€ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸
            consistency_score = self._check_rag_consistency(response, rag_context)
            score = (context_confidence * 0.6 + consistency_score * 0.4)

        # ë¹„ì „ ë¶„ì„ ê²°ê³¼ì™€ì˜ ì¼ì¹˜ì„±
        vision_result = context.get('vision_result', {})
        if vision_result:
            vision_confidence = vision_result.get('confidence', 0.5)
            vision_consistency = self._check_vision_consistency(response, vision_result)
            score = (score * 0.7 + (vision_confidence * vision_consistency) * 0.3)

        return round(score, 3)

    def _evaluate_completeness(self, prompt: str, response: str, context: Dict) -> float:
        """ì™„ì„±ë„ í‰ê°€"""

        # í•„ìˆ˜ ìš”ì†Œ ì²´í¬ë¦¬ìŠ¤íŠ¸
        required_elements = {
            'fee_information': ['ìˆ˜ìˆ˜ë£Œ', 'ìš”ê¸ˆ', 'ë¹„ìš©'],
            'schedule_information': ['ë°°ì¶œ', 'ì¼ì •', 'ìš”ì¼'],
            'application_process': ['ì‹ ì²­', 'ë°©ë²•', 'ì ˆì°¨'],
            'contact_information': ['ë¬¸ì˜', 'ì—°ë½ì²˜', 'ì „í™”']
        }

        found_elements = 0
        total_elements = len(required_elements)

        for element_type, keywords in required_elements.items():
            if any(keyword in response for keyword in keywords):
                found_elements += 1

        # ì‘ë‹µ ê¸¸ì´ ê¸°ë°˜ ë³´ì •
        response_length = len(response)
        length_score = min(response_length / 300, 1.0)  # 300ì ê¸°ì¤€

        completeness_score = (found_elements / total_elements) * 0.8 + length_score * 0.2

        return round(completeness_score, 3)

    def _evaluate_clarity(self, response: str) -> float:
        """ëª…í™•ì„± í‰ê°€"""

        score = 0.5

        # ë¬¸ì¥ ê¸¸ì´ ë¶„ì„
        sentences = response.split('.')
        avg_sentence_length = sum(len(s.split()) for s in sentences) / len(sentences) if sentences else 0

        if 10 <= avg_sentence_length <= 25:  # ì ì ˆí•œ ë¬¸ì¥ ê¸¸ì´
            score += 0.2

        # ì „ë¬¸ìš©ì–´ ì‚¬ìš© ë¹ˆë„
        technical_terms = ['ì¡°ë¡€', 'ê·œì •', 'ë²•ë ¹', 'ê³¼íƒœë£Œ']
        technical_ratio = sum(1 for term in technical_terms if term in response) / len(technical_terms)

        if technical_ratio < 0.3:  # ê³¼ë„í•œ ì „ë¬¸ìš©ì–´ ì‚¬ìš© ì§€ì–‘
            score += 0.1

        # êµ¬ì¡°í™” ì •ë„ (ë²ˆí˜¸, ë¶ˆë¦¿ í¬ì¸íŠ¸ ë“±)
        structure_indicators = ['1.', '2.', 'â€¢', '-', 'âœ“']
        if any(indicator in response for indicator in structure_indicators):
            score += 0.2

        return min(score, 1.0)

    def _evaluate_user_satisfaction(self, user_feedback: Dict) -> float:
        """ì‚¬ìš©ì ë§Œì¡±ë„ í‰ê°€"""

        satisfaction = user_feedback.get('overall_satisfaction', 3) / 5.0

        # í”¼ë“œë°± ì˜ê²¬ ë¶„ì„
        comments = user_feedback.get('additional_comments', '')
        if comments:
            positive_words = ['ì¢‹ì€', 'ë„ì›€', 'ëª…í™•', 'ì •í™•', 'ê°ì‚¬']
            negative_words = ['ë¶€ì¡±', 'í‹€ë¦°', 'ì–´ë ¤ìš´', 'ë³µì¡', 'ì´í•´']

            positive_count = sum(1 for word in positive_words if word in comments)
            negative_count = sum(1 for word in negative_words if word in comments)

            sentiment_score = (positive_count - negative_count) / max(positive_count + negative_count, 1)
            satisfaction = (satisfaction * 0.7 + (sentiment_score + 1) / 2 * 0.3)

        return round(satisfaction, 3)

    def generate_quality_report(self, period: str = 'weekly') -> Dict[str, Any]:
        """í’ˆì§ˆ ë³´ê³ ì„œ ìƒì„±"""

        report = {
            'period': period,
            'summary_statistics': {
                'total_evaluations': 0,
                'average_quality_score': 0.0,
                'quality_grade_distribution': {},
                'top_performing_prompts': [],
                'improvement_needed_areas': []
            },
            'trend_analysis': {
                'quality_trend': 'stable',  # 'improving', 'declining', 'stable'
                'trend_percentage': 0.0
            },
            'recommendations': []
        }

        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ê¸°ê°„ë³„ ë°ì´í„° ì¡°íšŒ ë° ë¶„ì„

        return report
```

## ğŸ—„ï¸ ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆ

### í”„ë¡¬í”„íŠ¸ ìµœì í™” ê´€ë ¨ í…Œì´ë¸”

```sql
-- A/B í…ŒìŠ¤íŠ¸ í…Œì´ë¸”
CREATE TABLE ab_tests (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),

    test_name TEXT NOT NULL,
    description TEXT,
    target_metric TEXT NOT NULL, -- 'user_satisfaction', 'response_accuracy', 'engagement'

    -- í…ŒìŠ¤íŠ¸ ì„¤ì •
    variations JSONB NOT NULL, -- ë³€í˜• ì„¤ì •
    traffic_split JSONB NOT NULL, -- íŠ¸ë˜í”½ ë¶„í•  ë¹„ìœ¨

    -- ìƒíƒœ ë° ì¼ì •
    status TEXT DEFAULT 'draft', -- 'draft', 'active', 'paused', 'completed'
    start_date TIMESTAMP,
    end_date TIMESTAMP,

    -- ê²°ê³¼
    results JSONB,
    statistical_analysis JSONB,

    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    created_by TEXT DEFAULT 'system'
);

-- A/B í…ŒìŠ¤íŠ¸ ì°¸ê°€ì í…Œì´ë¸”
CREATE TABLE ab_test_participants (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),

    test_id UUID REFERENCES ab_tests(id) ON DELETE CASCADE,
    user_session_id TEXT NOT NULL,

    -- í• ë‹¹ ì •ë³´
    variation_name TEXT NOT NULL,
    assigned_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,

    -- ìƒí˜¸ì‘ìš© ì´ë ¥
    interactions JSONB, -- [{type, timestamp, metrics}]

    UNIQUE(test_id, user_session_id)
);

-- ì‘ë‹µ í’ˆì§ˆ í‰ê°€ í…Œì´ë¸”
CREATE TABLE response_quality_evaluations (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),

    -- ì—°ê²° ì •ë³´
    user_session_id TEXT,
    prompt_strategy TEXT, -- PromptStrategy enum value
    ab_test_id UUID REFERENCES ab_tests(id),

    -- í”„ë¡¬í”„íŠ¸ ë° ì‘ë‹µ
    prompt_text TEXT NOT NULL,
    response_text TEXT NOT NULL,
    context_data JSONB,

    -- í’ˆì§ˆ í‰ê°€ ê²°ê³¼
    overall_score FLOAT NOT NULL,
    detailed_scores JSONB NOT NULL, -- {metric: score}
    quality_grade TEXT, -- 'excellent', 'good', 'fair', 'poor'

    -- ì‚¬ìš©ì í”¼ë“œë°± (ìˆëŠ” ê²½ìš°)
    user_feedback JSONB,

    -- ë©”íƒ€ë°ì´í„°
    evaluation_method TEXT DEFAULT 'automated', -- 'automated', 'manual', 'hybrid'
    evaluation_confidence FLOAT DEFAULT 0.5,

    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- í”„ë¡¬í”„íŠ¸ ì„±ëŠ¥ ì§€í‘œ í…Œì´ë¸”
CREATE TABLE prompt_performance_metrics (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),

    date DATE NOT NULL,
    prompt_strategy TEXT NOT NULL,

    -- ì„±ëŠ¥ ì§€í‘œ
    total_uses INTEGER DEFAULT 0,
    avg_quality_score FLOAT DEFAULT 0.0,
    avg_user_satisfaction FLOAT DEFAULT 0.0,
    avg_response_time_ms INTEGER DEFAULT 0,

    -- ì‚¬ìš©ì ë°˜ì‘
    positive_feedback_count INTEGER DEFAULT 0,
    negative_feedback_count INTEGER DEFAULT 0,
    improvement_suggestions_count INTEGER DEFAULT 0,

    -- ì •í™•ë„ ê´€ë ¨
    accuracy_rate FLOAT DEFAULT 0.0,
    rag_consistency_rate FLOAT DEFAULT 0.0,

    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,

    UNIQUE(date, prompt_strategy)
);

-- í”„ë¡¬í”„íŠ¸ ìµœì í™” ì‹¤í—˜ ë¡œê·¸
CREATE TABLE prompt_optimization_logs (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),

    optimization_type TEXT NOT NULL, -- 'ab_test', 'strategy_change', 'parameter_tuning'
    description TEXT,

    -- ë³€ê²½ ì „í›„
    before_config JSONB,
    after_config JSONB,

    -- ê²°ê³¼
    performance_impact JSONB, -- ì„±ëŠ¥ ë³€í™” ì§€í‘œ
    success BOOLEAN DEFAULT FALSE,

    -- ë©”íƒ€ë°ì´í„°
    triggered_by TEXT DEFAULT 'automated', -- 'automated', 'manual', 'scheduled'
    applied_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    applied_by TEXT DEFAULT 'system'
);
```

### ì¸ë±ìŠ¤ ìµœì í™”

```sql
-- ì„±ëŠ¥ ìµœì í™” ì¸ë±ìŠ¤
CREATE INDEX idx_ab_tests_status ON ab_tests(status);
CREATE INDEX idx_ab_test_participants_session ON ab_test_participants(user_session_id);
CREATE INDEX idx_response_quality_strategy ON response_quality_evaluations(prompt_strategy);
CREATE INDEX idx_response_quality_created ON response_quality_evaluations(created_at);
CREATE INDEX idx_prompt_metrics_date ON prompt_performance_metrics(date);
CREATE INDEX idx_prompt_metrics_strategy ON prompt_performance_metrics(prompt_strategy);
CREATE INDEX idx_optimization_logs_type ON prompt_optimization_logs(optimization_type);
```

## ğŸš€ êµ¬í˜„ ê°€ì´ë“œ

### 1ë‹¨ê³„: ê¸°ë³¸ ì»¨í…ìŠ¤íŠ¸ ì¸ì‹ í”„ë¡¬í”„íŠ¸ (2-3ì¼)

```python
# ServiceFactoryì— í”„ë¡¬í”„íŠ¸ ìµœì í™” ì„œë¹„ìŠ¤ë“¤ ë“±ë¡
registry.register_service(
    name='context_aware_prompt_service',
    service_class=type('ContextAwarePromptService', (), {}),
    module_path='src.domains.prompts.services.context_aware_prompt_service',
    dependencies=['feedback_service', 'district_rag_service'],
    is_optional=False,
    singleton=True
)
```

### 2ë‹¨ê³„: ì‘ë‹µ í’ˆì§ˆ í‰ê°€ ì‹œìŠ¤í…œ (2-3ì¼)

```python
# ê¸°ì¡´ ì‘ë‹µ ìƒì„± í”Œë¡œìš°ì— í’ˆì§ˆ í‰ê°€ ì¶”ê°€
def enhanced_response_generation(user_query, context):
    """í’ˆì§ˆ í‰ê°€ê°€ í¬í•¨ëœ ì‘ë‹µ ìƒì„±"""

    # 1. ì»¨í…ìŠ¤íŠ¸ ì¸ì‹ í”„ë¡¬í”„íŠ¸ ìƒì„±
    prompt_service = get_service('context_aware_prompt_service')
    prompt_result = prompt_service.generate_contextual_prompt(context)

    # 2. LLM ì‘ë‹µ ìƒì„±
    openai_service = get_service('openai_service')
    response = openai_service.generate_response(prompt_result['prompt'])

    # 3. ì‘ë‹µ í’ˆì§ˆ í‰ê°€
    quality_service = get_service('response_quality_service')
    quality_evaluation = quality_service.evaluate_response_quality(
        prompt=prompt_result['prompt'],
        response=response,
        context=context
    )

    # 4. ë‚®ì€ í’ˆì§ˆì¸ ê²½ìš° ì¬ìƒì„± (ì„ íƒì )
    if quality_evaluation['overall_score'] < 0.6:
        # ë‹¤ë¥¸ ì „ëµìœ¼ë¡œ ì¬ì‹œë„
        alternative_prompt = prompt_service.generate_contextual_prompt(
            context, fallback_strategy=True
        )
        response = openai_service.generate_response(alternative_prompt['prompt'])

    return {
        'response': response,
        'quality_score': quality_evaluation['overall_score'],
        'prompt_strategy': prompt_result['strategy'],
        'metadata': quality_evaluation
    }
```

### 3ë‹¨ê³„: A/B í…ŒìŠ¤íŠ¸ ì‹œìŠ¤í…œ (3-4ì¼)

```python
# A/B í…ŒìŠ¤íŠ¸ í†µí•©
def ab_test_integrated_response(user_session_id, user_query, context):
    """A/B í…ŒìŠ¤íŠ¸ê°€ í†µí•©ëœ ì‘ë‹µ ìƒì„±"""

    ab_test_service = get_service('ab_test_service')

    # 1. ì‚¬ìš©ìë¥¼ í™œì„± í…ŒìŠ¤íŠ¸ì— í• ë‹¹
    test_assignment = ab_test_service.assign_user_to_test(user_session_id)

    # 2. í• ë‹¹ëœ ë³€í˜•ì— ë”°ë¥¸ í”„ë¡¬í”„íŠ¸ ìƒì„±
    if test_assignment['assigned']:
        # í…ŒìŠ¤íŠ¸ ë³€í˜• ì ìš©
        context['ab_test_variation'] = test_assignment['variation']

    # 3. ì‘ë‹µ ìƒì„±
    response_result = enhanced_response_generation(user_query, context)

    # 4. í…ŒìŠ¤íŠ¸ ìƒí˜¸ì‘ìš© ê¸°ë¡
    if test_assignment['assigned']:
        ab_test_service.record_test_interaction(
            user_session_id=user_session_id,
            test_id=test_assignment['test_id'],
            interaction_type='response_generated',
            metrics={
                'quality_score': response_result['quality_score'],
                'response_length': len(response_result['response']),
                'prompt_strategy': response_result['prompt_strategy']
            }
        )

    return response_result
```

### 4ë‹¨ê³„: ì§€ì†ì  ìµœì í™” (ongoing)

```python
# ìë™ ìµœì í™” ìŠ¤ì¼€ì¤„ëŸ¬
def automated_optimization_scheduler():
    """ìë™ ìµœì í™” ìŠ¤ì¼€ì¤„ëŸ¬ (í¬ë¡ ì¡ìœ¼ë¡œ ì‹¤í–‰)"""

    optimization_service = get_service('prompt_optimization_service')

    # 1. ì£¼ê°„ ì„±ëŠ¥ ë¶„ì„
    weekly_analysis = optimization_service.analyze_weekly_performance()

    # 2. ê°œì„ ì´ í•„ìš”í•œ ì˜ì—­ ì‹ë³„
    improvement_areas = optimization_service.identify_improvement_areas(weekly_analysis)

    # 3. ìë™ A/B í…ŒìŠ¤íŠ¸ ì œì•ˆ
    for area in improvement_areas:
        if area['confidence'] > 0.8:  # ë†’ì€ ì‹ ë¢°ë„ì¸ ê²½ìš°ë§Œ
            test_proposal = optimization_service.propose_ab_test(area)

            # ìë™ ìŠ¹ì¸ ì¡°ê±´ í™•ì¸
            if test_proposal['risk_level'] == 'low':
                ab_test_service.create_ab_test(**test_proposal['config'])

    # 4. ì„±ëŠ¥ ë³´ê³ ì„œ ìƒì„±
    report = optimization_service.generate_optimization_report()

    # 5. ì•Œë¦¼ ë°œì†¡ (ê´€ë¦¬ìì—ê²Œ)
    send_optimization_report(report)
```

## ğŸ“Š ì„±ê³µ ì§€í‘œ ë° ëª¨ë‹ˆí„°ë§

### í•µì‹¬ KPI
1. **ì‘ë‹µ í’ˆì§ˆ**: ì „ì²´ í‰ê·  í’ˆì§ˆ ì ìˆ˜ > 4.5/5.0
2. **A/B í…ŒìŠ¤íŠ¸ íš¨ìœ¨**: ì›” 2-3ê°œ í…ŒìŠ¤íŠ¸, ê°œì„ ìœ¨ > 20%
3. **ì‚¬ìš©ì ë§Œì¡±ë„**: í”„ë¡¬í”„íŠ¸ ë§Œì¡±ë„ > 4.5/5.0
4. **ì‹œìŠ¤í…œ ì™„ì„±ë„**: ëª¨ë“  Phase í†µí•© íš¨ê³¼ > 80%

### ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ
- ì‹¤ì‹œê°„ ì‘ë‹µ í’ˆì§ˆ ì§€í‘œ
- A/B í…ŒìŠ¤íŠ¸ ì§„í–‰ í˜„í™© ë° ê²°ê³¼
- í”„ë¡¬í”„íŠ¸ ì „ëµë³„ ì„±ëŠ¥ ë¹„êµ
- ì‚¬ìš©ì í”¼ë“œë°± íŒ¨í„´ ë¶„ì„
- ì§€ì†ì  ê°œì„  íŠ¸ë Œë“œ

## ğŸ”® ì „ì²´ ì‹œìŠ¤í…œ ì™„ì„±

### Phase í†µí•© ì‹œë„ˆì§€
```python
# ëª¨ë“  Phaseê°€ í†µí•©ëœ ìµœì¢… í”Œë¡œìš°
def complete_ecoguide_flow(image, user_location, user_session):
    """ì™„ì „í•œ EcoGuide ë¶„ì„ í”Œë¡œìš°"""

    # Phase 0: ê¸°ì¡´ ì´ë¯¸ì§€ ë¶„ì„
    vision_result = vision_service.analyze_image_pipeline(image)

    # Phase 1: ì‚¬ìš©ì í”¼ë“œë°± ì¤€ë¹„
    feedback_context = prepare_feedback_context(vision_result)

    # Phase 3: RAG ê¸°ë°˜ ì§€ìì²´ ì •ë³´
    district_guidance = rag_service.get_district_guidance(
        query=f"{vision_result.category} ë°°ì¶œ ë°©ë²•",
        district_code=get_district_code(user_location),
        item_info=vision_result.to_dict()
    )

    # Phase 4: ìµœì í™”ëœ í”„ë¡¬í”„íŠ¸ë¡œ ìµœì¢… ì‘ë‹µ
    final_context = PromptContext(
        user_session_id=user_session.id,
        vision_result=vision_result.to_dict(),
        rag_context=district_guidance,
        user_history=user_session.history,
        # ... ê¸°íƒ€ ì»¨í…ìŠ¤íŠ¸
    )

    optimized_response = ab_test_integrated_response(
        user_session.id,
        "ëŒ€í˜•íê¸°ë¬¼ ë°°ì¶œ ì•ˆë‚´",
        final_context
    )

    # Phase 2: ë¼ë²¨ë§ ë°ì´í„° ì €ì¥
    labeling_service.save_analysis_data(
        original_image=image,
        vision_result=vision_result.to_dict(),
        final_response=optimized_response,
        context=final_context
    )

    return {
        'vision_analysis': vision_result,
        'district_guidance': district_guidance,
        'optimized_response': optimized_response,
        'feedback_interface': feedback_context,
        'overall_confidence': calculate_system_confidence(
            vision_result, district_guidance, optimized_response
        )
    }
```

## ğŸ’¡ êµ¬í˜„ íŒ

1. **ë‹¨ê³„ì  ë„ì…**: ê¸°ë³¸ ì»¨í…ìŠ¤íŠ¸ ì¸ì‹ë¶€í„° ì‹œì‘í•˜ì—¬ ì ì§„ì ìœ¼ë¡œ ê³ ë„í™”
2. **ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§**: ëª¨ë“  ìµœì í™”ì˜ íš¨ê³¼ë¥¼ ì •ëŸ‰ì ìœ¼ë¡œ ì¸¡ì •
3. **ì‚¬ìš©ì ì¤‘ì‹¬**: ê¸°ìˆ ì  ì™„ì„±ë„ë³´ë‹¤ ì‚¬ìš©ì ê²½í—˜ ê°œì„ ì— ì§‘ì¤‘
4. **ì§€ì†ì  ê°œì„ **: ìë™í™”ëœ ìµœì í™” ì‹œìŠ¤í…œìœ¼ë¡œ ì¥ê¸°ì  í’ˆì§ˆ ìœ ì§€

ì´ Phase 4ëŠ” **ëª¨ë“  Phaseì˜ ì„±ê³¼ë¥¼ í†µí•©í•˜ì—¬ ì™„ì „í•œ AI ì„œë¹„ìŠ¤**ë¥¼ ì™„ì„±í•˜ë©°, ì§€ì†ì ì¸ ìê°€ ê°œì„ ì„ í†µí•´ ì¥ê¸°ì ìœ¼ë¡œ ê²½ìŸ ìš°ìœ„ë¥¼ í™•ë³´í•©ë‹ˆë‹¤.